# Install Required Libraries
!pip install transformers accelerate langgraph faiss-cpu sentence-transformers

# Import Libraries
from langgraph.graph import StateGraph
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import faiss
import torch

# Load Dataset and Index
print("Loading MedMCQA dataset and FAISS index...")
df = pd.read_csv("medmcqa_dataframe.csv")
index = faiss.read_index("medmcqa_index.faiss")
embeddings = np.load("medmcqa_embeddings.npy")

# Load Embedding Model for FAISS queries
encoder = SentenceTransformer("all-MiniLM-L6-v2")

# Load Hugging Face LLM (FLAN-T5)
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define Utility Functions
def retrieve_from_faiss(query, k=1):
    vector = encoder.encode([query])
    distances, indices = index.search(np.array(vector), k)
    return indices[0], distances[0]

# Generate response with dataset-grounded output
def format_response(state):
    row = df.iloc[state["index"]]
    options = [row["opa"], row["opb"], row["opc"], row["opd"]]
    correct_idx = row["cop"]
    correct_option = ['A', 'B', 'C', 'D'][correct_idx]
    correct_text = options[correct_idx]

    # Optional columns: explanation, subject, topic
    explanation = str(row.get("exp", "")).strip()
    subject = str(row.get("subject_name", "")).strip()
    topic = str(row.get("topic_name", "")).strip()

    response = f"""Question:
{row['question']}

Options:
A. {options[0]}
B. {options[1]}
C. {options[2]}
D. {options[3]}

Correct Answer: {correct_option}. {correct_text}
"""
    if explanation:
        response += f"\nExplanation: {explanation}"
    if subject:
        response += f"\nSubject: {subject}"
    if topic:
        response += f"\nTopic: {topic}"
    return {"answer": response.strip()}

# Define LangGraph Nodes
def receive_query(state):
    return {"query": state["query"]}

def search_knowledge(state):
    query = state["query"]
    indices, distances = retrieve_from_faiss(query, k=1)
    confidence = 1 - distances[0]
    return {"index": indices[0], "confidence": confidence, "query": query}

def reject_if_not_confident(state):
    if state["confidence"] < 0.75:
        return {"answer": "I'm sorry, I couldn't find an answer to that in my dataset."}
    return state

# Build LangGraph with schema
from typing import TypedDict

class ChatState(TypedDict):
    query: str
    index: int
    confidence: float
    answer: str

chat_graph = StateGraph(ChatState)
chat_graph.add_node("Input", receive_query)
chat_graph.add_node("Search", search_knowledge)
chat_graph.add_node("Reject", reject_if_not_confident)
chat_graph.add_node("Respond", format_response)
chat_graph.set_entry_point("Input")
chat_graph.add_edge("Input", "Search")
chat_graph.add_edge("Search", "Reject")
chat_graph.add_edge("Reject", "Respond")
chatbot = chat_graph.compile()

# Interactive Chatbot Mode
while True:
    user_input = input("Ask a medical question (or type 'exit' to quit): ")
    if user_input.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break
    result = chatbot.invoke({"query": user_input})
    print("\nAnswer:\n")
    print(result["answer"])
    print("-" * 50)